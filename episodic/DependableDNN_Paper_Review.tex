\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{dependable_dnn}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\D}{\mathcal{D}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{} % *** Enter the Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Episodic Training for Domain Generalization
	 \\ {\rm {\normalsize Seungmin Lee (profile2697@gmail.com; 2013-11420), Dept. of Computer Science and Engineering, Seoul National University}}} 

\maketitle
\thispagestyle{empty}

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\section{Problem Setting}
This paper attempts to solve \textit{Domain Generalization} (DG). In DG, we assume that there are $n$ source domains $\D = [\D_1,\dots, \D_n]$ where $\D_i$ indicates $i$-th source domain which contains data-label pairs $\{x_i^j, y_i^j\}$. Using source domains, we try to learn a model that generalizes well to unseen target domain $D_t$. This paper deals with two types of DGs, one \textit{homogeneous DG} and the other \textit{heterogeneous DG}. The homogeneous DG assumes all domains, including target domain, share the same label space. Meanwhile, the heterogeneous DG assumes the domains can have different label space.
The authors used a model consists of a feature extractor $\theta$ and a classifier $\psi$. 

\section{Intuition}
Like meta-learning, the authors tried to design an episode that can closely simulate the testing condition. First, they exposed a feature extractor to poorly calibrated classifiers and then trained the feature extractor to work well on those classifiers. We can interpret this training procedure as training the feature extractor to encode more general information that can be used for all classifiers. Besides, they trained a classifier with poorly calibrated feature extractors. This procedure can be understood as training the classifier to be robust to noise generated by feature extractors.

\section{Methods}
The proposed method uses little bit complicating training procedures but basically uses a weighted sum of losses described in this section.

\vspace{-0.2cm}
\paragraph{Vanilla Aggregation Method}
Aggregation Method is simple yet effective baseline. We just aggregate data-label pairs from all source domains and train a model using those pairs. The loss can be written as follows:

\begin{equation}
\label{eq:agg}
\begin{aligned}
L_{agg} = \underset{\theta, \psi}{\operatorname{argmin}}~ \mathbb{E}_{\mathcal{D}_i\sim\mathcal{D}} \big[ \mathbb{E}_{(\mathbf{x}_i,y_i)\sim \mathcal{D}_i} \big[  \ell(y_i, \psi (\theta(\mathbf{x}_i)) \big] \big]
\end{aligned}
\end{equation}

\vspace{-0.4cm}
\paragraph{Domain-Specific Models} For creating poorly calibrated feature extractors and classifiers, the authors trained domain-specific models trained only on their domain using Eq.~\ref{eq:dsnn},

\begin{equation}
\label{eq:dsnn}
\begin{aligned}
L_{ds} = \underset{\theta_i, \psi_i}{\operatorname{argmin}}~  \mathbb{E}_{(\mathbf{x}_i, y_i)\sim \mathcal{D}_i} \big[  \ell(y_i, \psi_i (\theta_i(\mathbf{x}_i)) \big]
\end{aligned}
\end{equation}

where $\ell$ is cross entropy loss.

\paragraph{Episodic Training for Feature Extractor}
To get a robust feature extractor, the authors trained a feature extractor to work well with other domains' classifiers. If the classifiers are vulnerable to texture or style, the feature extractor should learn not to extract those kinds of noise. This procedure makes the feature extractor to encode general and discriminative features. This procedure can be written as follows:

\begin{equation}
\begin{aligned}
\label{eq:agg-reg-feat}
L_{epif} = \underset{\theta}{\operatorname{argmin}}~
\mathbb{E}_{i,j\sim[1,n], i\neq j} \big[ \mathbb{E}_{(\mathbf{x}_i,y_i)\sim \mathcal{D}_i} \big[  \ell(y_i, \overline{\psi}_j(\theta(\mathbf{x}_i)) \big] \big]
\end{aligned}
\end{equation}

where $\overline{\psi}_j$ means a fixed classifier of $j$-th domain. Fixing weights is crucial because it prevents weights to be polluted by other domains.

\paragraph{Episodic Training for Classifier}
Similar to \textbf{Episodic Training for Feature Extractor}, we can train a robust classifier using other domains' feature extractors. If the feature extractors encode noisy information like style or texture, the classifier learns not to use those kinds of information. This training makes the classifier robust to noise from feature extractors. This procedure using following loss:

\begin{equation}
\begin{aligned}
\label{eq:agg-reg-clf}
L_{epic} = \underset{\psi}{\operatorname{argmin}}~
\mathbb{E}_{i,j\sim[1,n], i\neq j} \big[ \mathbb{E}_{(\mathbf{x}_i,y_i)\sim \mathcal{D}_i} \big[  \ell(y_i, \psi(\overline{\theta}_j(\mathbf{x}_i)) \big] \big]
\end{aligned}
\end{equation}

where $\overline{\theta}_j$ is fixed feature extractor.

\paragraph{Episodic Training by Random Classifier}
The episodic training procedures described above are limited to homogeneous DG. For heterogeneous DG, the authors proposed episodic training using a random classifier. The use of random classifier is an extreme version of a poorly calibrated classifier. This procedure uses a loss described below:

\begin{equation}
\begin{aligned}
\label{eq:agg-fclf}
L_{epir} = \underset{\theta}{\operatorname{argmin}}~
\mathbb{E}_{\mathcal{D}_i\sim\mathcal{D}} \big[ \mathbb{E}_{(\mathbf{x}_i,y_i)\sim \mathcal{D}_i} \big[  \ell(y_i, \overline{\psi}_r(\theta(\mathbf{x}_i)) \big] \big]
\end{aligned}
\end{equation} 

where $\overline{\psi}_r$ is a random classifier with fixed weights.

\section{Personal Memo}
There must be a better way to create a poorly calibrated sets of feature extractors and classifiers. Despite ablation study was given, it is hard to understand why the random classifier is helpful to DG. Additionally, I wonder why the difference between coefficients of $L_{epir}$ and $L_{epif}$ is up to 20 times.
 

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%


{\small
\bibliographystyle{ieee}
%\bibliography{egbib}
}

\end{document}
