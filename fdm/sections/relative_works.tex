\subsection{Multi-Domain Learning and Multi-Source Domain Adaptation}
The primary purpose of Multi-Domain Learning (\textit{MDL}) is to learn a single model that can compactly represent all domains with a smaller number of parameters~\cite{yang2015mdlmtl, Rebuffi17, Bilen17, Rebuffi18}. For this purpose, Bilen~\etal~\cite{Bilen17} adopts domain-specific parameters of instance normalization and batch normalization while using shared parameters in other layers. Rebuffi~\etal~\cite{Rebuffi17} transforms the standard residual network architecture to share a significant amount of parameters between different domains.

Multi-Source Domain Adaptation (\textit{MSDA}) also uses a set of source domains, but it additionally utilizes the images of an unlabeled target domain. The main focus of MSDA is to train a model that works well on the target domain without labels of it. Even though many studies have been conducted on single-source domain adaptation, there are a limited number of researches on MSDA~\cite{Zhao2018NIPS, Chang2019cvpr, guo2018-multi, peng2018moment}. Chang~\etal~\cite{Chang2019cvpr} proposes a domain-specific batch normalization with shared weights parameters and extends their method to MSDA. Peng~\etal~\cite{peng2018moment} suggests a way to reduce the moment distance between different source domains as well as reducing the distance between target and source domains. The moment distance measures the difference between feature distributions of two domains without concerning the task at hand.

MDL and MSDA are closely related to DG since DG also utilizes a set of domains as training data. However, DG is different from MDL in that the primary focus of DG is to learn semantic and domain-invariant features, not to learn compact representations. Furthermore, DG is more challenging than MSDA because the target domain is totally unavailable in DG.


\subsection{Domain Generalization}
Even though existing DG methods basically aim to learn domain-invariant features, the methods can be classified into several groups based on their approaches. The first group proposes a novel architecture~\cite{Khosla12undobias, Li2017dg}. The methods separate domain-specific parameters and domain-agnostic parameters. After that, they only extract and utilize the domain-agnostic parameters for the unseen domain. The second group of methods suggests optimization algorithms that adopt episodic learning or self-supervised learning~\cite{li2019episodic, Li2018MLDG, NIPS2018_metareg, carlucci2019domain}. For example, MLDG~\cite{Li2018MLDG} constructs an episode by splitting the source domains into training domains and test domains in each iteration. The final group of methods uses losses that aims to learn domain-invariant features~\cite{Ghifary2015mtae, muandet2013domaingeneralization, mmdaaecvpr2018}. These methods often adopt maximum mean distribution (\textit{MMD}) constraints. However, MMD simply tries to match the feature distributions of all available source domains without concerning the task at hand. Therefore, methods that adopted MMD can be sub-optimal~\cite{Saito2018, Saito2018b}. Otherwise, we propose a regularizing method using a simple consistency loss that explicitly utilizes the task-specific information. By using this simple loss, we expect that the model can learn domain-invariant but semantic features.