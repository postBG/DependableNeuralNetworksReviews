\subsection{Multi-Domain Learning and Multi-Source Domain Adaptation}
The primary purpose of Multi-Domain Learning (\textit{MDL}) is to learn a single model that can compactly represent all domains with a smaller number of parameters~\cite{yang2015mdlmtl, Rebuffi17, Bilen17, Rebuffi18}. For this purpose, Bilen~\etal~\cite{Bilen17} adopts shared model parameters except for batch normalization parameters and instance normalization parameters. Rebuffi~\etal~\cite{Rebuffi17} transforms the standard residual network architecture to share a significant amount of parameters between different domains.

Multi-Source Domain Adaptation (\textit{MSDA}) also uses a set of domains, but it additionally utilizes the images of an unlabeled target domain. The main focus of MSDA is to train a model that works well on the target domain without labels of it. Even though many studies have been conducted on single-source domain adaptation, there are a limited number of researches on MSDA~\cite{Zhao2018NIPS, Chang2019cvpr, guo2018-multi, peng2018moment}. Chang~\etal~\cite{Chang2019cvpr} proposes a domain-specific batch normalization with shared weights parameters and extends their method to MSDA. Peng~\etal~\cite{peng2018moment} suggests reducing moment distances between different domains. The moment distance measures the difference between feature distributions of two domains without concerning the task at hand.

MDL and MSDA are closely related to DG since DG also utilizes many sources in many cases. However, DG is different from MDL in that the primary focus of the DG is to learn semantic and domain-invariant features, not to learn compact representations. Moreover, DG is more challenging than MSDA because DG can not utilize the target domain in training.


\subsection{Domain Generalization}
Even though existing DG methods basically aim to learn domain-invariant features, these can be classified into several groups based on their strategies. The first group proposes a novel architecture~\cite{Khosla12undobias, Li2017dg}. The methods basically separate domain-specific parameters and domain-agnostic parameters. After that, they only extract and utilize the domain-agnostic parameters for the unseen domain. The second group of methods suggests optimization algorithms that adopt meta-learning or episodic learning~\cite{li2019episodic, Li2018MLDG, NIPS2018_metareg}. For example, MLDG~\cite{Li2018MLDG} constructs an episode by splitting the source domains into training domains and test domains in each iteration. The final group of methods uses losses that aims to learn domain-invariant features~\cite{Ghifary2015mtae, muandet2013domaingeneralization, mmdaaecvpr2018}. The losses such as maximum mean distribution (\textit{MMD}) just try to match the feature distributions of all available source domains without concerning the task at hand. Therefore, methods that adopted MMD show sub-optimal performances~\cite{Saito2018, Saito2018b}. We propose a regularizing method using a simple consistency loss that explicitly utilizes the task-specific information. By using this simple loss, we expect that the model can learn domain-invariant but semantic features.