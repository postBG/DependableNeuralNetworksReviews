\subsection{Brief Summary of Datasets}
In this section, we give summaries of datasets used in our experiments. We use two different object classification datasets: \textbf{VLCS}~\cite{chen2013vlcs} and \textbf{PACS}~\cite{Li2017dg}. VLCS dataset consists of four famous datasets which are PASCAL VOC2007 (V)~\cite{Everingham10}, Labelme (L)~\cite{labelme}, Caltech 101 (C)~\cite{Feifei2004caltech}, and SUN09 (S)~\cite{Choi2010sun09}. Otherwise, PACS are composed of four datasets that have more significant visual domain gaps than those of VLCS. PACS consists of Photo (P), Art painting (A), Cartoon (C), and Sketch (S).

\subsection{Settings}
In this section, we evaluate the proposed method on standard DG benchmarks. For datasets, we use PACS~\cite{Li2017dg} and VLCS~\cite{chen2013vlcs}, which are commonly used for demonstrating DG algorithms. For networks, we adopt Alexnet~\cite{Krizhevsky2012} and ResNet-18~\cite{He2016resnet}. We found the architectures of Alexnet~\cite{Krizhevsky2012} used in each paper are different. Thus, we clarify the used structure in this paper is the same as Li~\etal~\cite{li2019episodic}. In each network, we use convolution layers as the feature extractor and fully-connected layers as the classifier. For hyperparameters, we set $\lambda_{max}$ to 0.1 and $T_r$ to 40 for all experiments. We train a model for 100 epochs and use the batch size 512 for all settings. Following the experiments' protocol used in Li~\etal~\cite{Li2017dg}, we evaluate the performance on a validation set during each epoch to get the best model, and we re-evaluate the performance on a test set using the model. We denote the proposed method as FDM in the results tables.


\subsection{Results on PACS + Alexnet}
The experimental results on PACS + Alexnet can be seen on Table~\ref{tab:pacs}. The proposed method shows a +1.7\% improvement compared to the Deep-All baseline, which indicates the proposed method could be helpful to DG tasks. Furthermore, In this setting, the proposed method shows better performance than the majority of the previous methods. Interestingly, the Deep-All baseline also exhibits better performance than many previous works, as mentioned in Li~\etal~\cite{li2019episodic}. Importantly, the proposed method shows better performance than all methods trying to match feature distribution without concerning the task at hand~\cite{ghifary2015domain, ganin2016dann, muandet2013domaingeneralization}. The proposed method also shows the best performance when the target domain is 'cartoon.'

\subsection{Results on VLCS + Alexnet}
To demonstrate the proposed method works well on another dataset, We evaluate FDM on VLCS + Alexnet. The results are on Table~\ref{tab:vlcs}. First, we could not reproduce the baseline of other works. Our baseline has almost 6\% lower performance than the Deep-All from Li~\etal~\cite{li2019episodic}. Therefore, we emphasize that the performance of our method has a large disadvantage than other methods in advance.  In this setting, FDM shows better performance than the baseline model. However, the gain is only +0.7\%, which is quite marginal. These results indicate the proposed regularization could be a hard constraint. We will discuss this problem at \ref{discussion}. Still, the proposed method shows better performance than ~\cite{lresvm, muandet2013domaingeneralization}. However, it shows weak performance than others. The disadvantage mentioned above seems to be a big cause, so it could not be fair comparisons.

\subsection{Results on PACS + ResNet-18}
We evaluate the proposed method on PACS + ResNet-18 to test whether FDM works well using different network architecture. The estimated results are on table~\ref{tab:pacs_resnet}. The proposed method consistently shows better performance than the Deep-All baseline. However, the gain is only +0.5\%, which indicates the proposed method can be redundant when the network structure is strong enough. Moreover, the proposed method only gives better performance than CrossGrad~\cite{shankar2018generalizing}. Nevertheless, the proposed method shows the best performance when the target domain is 'photo.' Interestingly, DANN~\cite{ganin2016dann} designed to match the feature distribution without concerning the task at hand also shows better performance than FDM and shows comparable performance than state-of-the-art methods. It indicates if the network is strong enough, and there is a set of source domains, a simple feature distribution matching would be enough. 

\begin{table*}[t]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|cccccccc|ccc}
			\toprule
			Src. & Trgt. & D-MTAE~\cite{Ghifary2015mtae} & LRE-SVM~\cite{lresvm} & CrossGrad~\cite{shankar2018generalizing} & DICA~\cite{muandet2013domaingeneralization}   &  DANN~\cite{ganin2016dann} & MetaReg~\cite{NIPS2018_metareg} & MLDG~\cite{Li2018MLDG} &  Epi-FCR~\cite{li2019episodic} & Deep-All (\cite{li2019episodic}) & Deep-All & FDM \\
			\midrule
			L,C,S&V& 63.9& 60.6 & 65.5 & 63.7& 66.4 & 65.0 & 67.7 & 67.1 & 65.4 & 59.7 & 59.3 \\
			V,C,S&L& 60.1& 59.7 & 60.0 & 58.2 & 64.0 & 60.2 & 61.3& 64.3 & 60.6 & 54.1 & 54.3\\
			V,L,S&C& 89.1 & 88.1 & 92.0 & 79.7 & 92.6&  92.3 & 94.4 & 94.1 & 93.1 & 87.7 & 88.0 \\
			V,L,C&S& 61.3& 54.9 & 64.7 & 61.0 & 63.6 & 64.2 & 65.9 & 65.9 & 65.8 & 61.4 & 64.0 \\
			\midrule
			\multicolumn{2}{c|}{Ave.}& 68.6 &65.8 &  70.5 & 65.7 & 71.7& 70.4 & 72.3 & 72.9 & 71.2 & 65.7 & 66.4\\
			\bottomrule
	\end{tabular}}
	\vspace{-0.2cm}
	\caption{\small Cross-domain object classification results (accuracy. \%) on VLCS using AlexNet.}
	% \vspace{-0.4cm}
	\label{tab:vlcs}
\end{table*}

\begin{table*}[t]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|ccccc|ccc}
			\toprule
			Src. & Trgt. & CrossGrad~\cite{shankar2018generalizing}&  DANN~\cite{ganin2016dann} & MetaReg~\cite{NIPS2018_metareg} & MLDG~\cite{Li2018MLDG} &  Epi-FCR~\cite{li2019episodic} & Deep-All & FDM \\
			\midrule
			P,C,S&A&  78.7 & 81.3 & 79.5& 79.5 & 82.1 &  77.3 & 77.7 \\
			P,A,S&C& 73.3 & 73.8 & 75.4&  77.3 & 77.0& 73.8 & 74.2\\
			A,C,S&P& 94.0 & 94.0& 94.3&  94.3 & 93.9 & 94.1 & 94.3 \\
			P,A,C&S& 65.1 & 74.3 & 72.2& 71.5 & 73.0 & 70.8 & 71.7 \\
			\midrule
			\multicolumn{2}{c|}{Ave.}& 77.8 & 80.8 & 80.4& 80.7& 81.5  & 79.0 & 79.5 \\
			\bottomrule
	\end{tabular}}
	\vspace{-0.3cm}
	\caption{\small Cross-domain object classification results (accuracy. \%) on PACS using ResNet-18.}
	\vspace{-0.4cm}
	% \vspace{-0.4cm}
	\label{tab:pacs_resnet}
\end{table*}