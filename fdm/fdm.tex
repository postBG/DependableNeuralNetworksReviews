\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[ruled]{algorithm}
% \usepackage[options]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subcaption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\newcommand{\D}{\mathcal{D}}
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\def\etal{\textit{et al.}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Class-wise Feature Distribution Matching Regularization for Domain Generalization}

\author{Seungmin Lee\\
Seoul National University\\
{\tt\small profile2697@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
 Domain generalization (\textit{DG}) aims to learn a model that generalizes well to an unseen domain (\textit{target domain}), which has a different distribution than known domains (\textit{source domain}). Many of the previous works try to learn domain-invariant features. These methods adopt a loss that tries to match the whole distributions of the source domains. However, these works are sub-optimal because they rarely utilize task-specific information, such as class labels. Concerning the information, we propose a simple but effective regularizing method called Class-wise Feature Distribution Matching (\textit{FDM}). The proposed method attempts to induce the network to produce similar features when the labels are the same. By doing this, a model is expected to learn more task-specific and robust features than the previous works.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\input{sections/intro.tex}

\section{Related Works}
\input{sections/relative_works.tex}

\section{Proposed Method}
\input{sections/method.tex}

\section{Experiments}
\input{sections/experiments}

\section{Future Works}
\input{sections/future_works.tex}

\section{Conclusion}
\input{section/conclusion.tex}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
