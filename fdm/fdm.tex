\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[ruled]{algorithm}
% \usepackage[options]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subcaption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\newcommand{\D}{\mathcal{D}}
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\def\etal{\textit{et al.}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Class-wise Feature Distribution Matching Regularization\\ for Domain Generalization}

\author{Seungmin Lee\\
Seoul National University\\
{\tt\small profile2697@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
 Domain generalization (\textit{DG}) aims to learn a model that generalizes well to an unseen domain (\textit{target domain}), which has a different distribution than known domains (\textit{source domains}). Many of the previous works try to learn domain-invariant features. These methods adopt a loss that tries to match the whole distributions of the source domains. However, these works are sub-optimal because they rarely utilize task-specific information, such as class labels. Concerning the information, we propose a simple regularizing method called Class-wise Feature Distribution Matching (\textit{FDM}). The proposed method induces a model to produce similar features when the labels of examples are the same, regardless of the examples' domains. By doing this, the model is expected to learn more task-specific and robust features than the previous works. To demonstrate the proposed methods, we conduct experiments on various settings. The proposed method consistently shows improvement compared to baseline. However, the analyses also reveal that domain-invariant features do not guarantee high performance.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\input{sections/intro.tex}

\section{Related Works}
\input{sections/relative_works.tex}

\section{Proposed Method}
\input{sections/method.tex}

\section{Experiments}
\input{sections/experiments}

\section{Future Works}
\input{sections/future_works.tex}

\section{Conclusion}
\input{section/conclusion.tex}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
