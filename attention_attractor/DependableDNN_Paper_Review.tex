\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{dependable_dnn}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{} % *** Enter the Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\newcommand{\Da}{\mathcal{D}_a}
\newcommand{\Db}{\mathcal{D}_b}
\newcommand{\Qa}{Q_a}
\newcommand{\Qb}{Q_b}
\newcommand{\Qab}{Q_{a+b}}
\newcommand{\Sa}{S_a}
\newcommand{\Sb}{S_b}
\newcommand{\Wa}{W_a}
\newcommand{\Wb}{W_b}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Incremental Few-Shot Learning with Attention Attractor Networks\\ {\rm {\normalsize Seungmin Lee (profile2697@gmail.com; 2013-11420), Dept. of Computer Science and Engineering, Seoul National University}}}   % **** Enter the paper title and student information here

\maketitle
\thispagestyle{empty}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\section{Introduction}
This paper tried to solve \textit{incremental few-shot learning}. In incremental few-shot learning, we assumes there are base classes and novel classes which are disjoint each other. A model is initially trained using examples from base classes with sufficient labels. Then without re-training on the base classes, we train the model on few-shot labeled novel classes. After trained on novel classes, performance of the model is evaluated on both base and novel classes. If we train the model naively, the performance on base classes is largely dropped (\textit{Catastrophic forgetting}). To avoid this problem, the authors proposed an \textbf{Attention Attractor Networks Regularizer} that helps the network not lose too much information about the base classes. They also showed the regularizer can be trained using recurrent back-propagation.

\section{Methods}
In this method, a model trained using examples from base classes in \textbf{Pretrain Stage}. Then the model is trained by repeating \textbf{Incremental Few-Shot Learning Stage} and \textbf{Meta-Learning Stage}. Additionally. there are four kinds of parameters; a feature extractor, a base classifier parameterized by $\Wa$, a novel classifier $\Wb$, and meta parameters $\theta_{E}$.

\subsection{Pretraining Stage}\label{pretrain}

This step aims to get a good feature extractor and a base classifier parameterized by $W_a$ before training on novel classes. Pretaining stage using typical cross entropy for the base dataset $\{(x_{a,i},y_{a,i})\}_{i=1}^{N_a}\in\Da$ where $y_{a,i}\in\{1...K\}$ and $x_{a, i}$ are $i$-th label and example from the base dataset $\Da$, respectively. 

\subsection{Incremental Few-Shot Learning Stage}\label{fewshot}

In this step, the model trained on a episode $\xi$ sampled from the novel dataset $\Db$ until convergence. The episode $\xi$ is composed of a support set $\Sb$ and a query set $\Qb$, which play the same role as training set and validation set in supervised learning. Additionally, the support set $\Sb$ only contains a few labeled samples. In this step, $\Wa$ is fixed and $\Wb$ is trained using following loss:

\begin{equation}
\label{eq:general_form}
L^{S}(\Wb, \theta_{E}) = cross\_entropy(\Wb, \Sb) + R(W_b,\theta_E).
\end{equation}

where $R(\cdot, \theta_E)$ is the \textbf{attention attractor networks regularizer} parameterized by $ \theta_E $. The parameters $\theta_{E}$ are fixed in this stage but trained in the meta-learning stage. The regularizer is used for alleviating catastrophic forgetting at base classes $\Da$. More details about the regularizer and $\theta_{E}$ are described in \textbf{Attention Attractor Networks Regularizer}

\paragraph{Attention Attractor Networks Regularizer}\label{atnr}
The Attention Attractor Networks Regularizer tries to make $\Wb$ encode the information about base classes which is contained in $\Wa$. The regularizer is defined as follows:

\begin{equation}
R(W_b,\theta_E) = \sum_{k'=1}^{K’} squared\_mahalanobis\_dist (W_{b, k’}, u_{k’}, \gamma)
\end{equation}

where $K'$ is number of novel classes, $W_{b, k'}$ is $k'$-th column of $\Wb$, $\gamma$ is a learnable parameter which is used for calculating mahalanobis distance, and $u_{k'}$ is \textit{attractor} of a novel class $k'$.

The attractor $u_{k'}$ is defined as follows:
\begin{equation}
u_{k'} = \sum_{k} a_{k', k}U_k + U_0
\end{equation}

where $U_k = f_\phi (W_{a, k})$ where $\phi$ is parameters of MLP  $f$. $U_k$ can be interpreted as a learned memory of the base class $k$. $U_0$ is a bias term, and $a_{k', k}$ is a normalized pairwise attention between a novel class $k'$ and a base class $k$. Meanwhile, $a_{k', k}$ has a learnable temparature scala $\tau$. Therefore, $ \theta_E $ consists of $\gamma, \phi, U_0$, and $ \tau$. These parameters are trained in meta-learning stage and are fixed in the incremental few-shot learning stage. Because $U_k$ is the memory of the base class k, $u_{k'}$ contains the information of the base classes. Thus, keeping $\Wb$ and $u_{k'}$ close, $R(W_b,\theta_E)$ induces $\Wb$ to encode the information that used for base classes.

\subsection{Meta-Learning Stage}\label{meta}
In this step, the meta parameters $ \theta_E $ are trained using the episode $\xi = \{\Sb, \Qb\}$ and $\Qa$ . $ \theta_E $ is trained to minimize the expected cross entropy loss for both $\Qa$ and $\Qb$. Therefore, in this stage, both base and novel classes are used. To train $\theta_{E}$, Recurrent Back-propagation is used because BPTT needs too many iterations until convergence, and T-BPTT can be unstable depending on T.

\section{Personal Memo}
Proposed method still needs base dataset for training a few novel classes. Trying to encode information of base classes to novel classifier seems unnatural.

{\small
\bibliographystyle{ieee}
}

\end{document}
