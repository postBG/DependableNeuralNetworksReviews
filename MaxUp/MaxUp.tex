\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{dependable_dnn}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage[table, dvipsnames]{xcolor}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{} % *** Enter the Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{MaxUp: A Simple Way to Improve Generalization of Neural Network Training\\ {\rm {\normalsize Seungmin Lee (profile2697@gmail.com; 2020-20866), \\Dept. of Electrical and Computer Engineering, Seoul National University}}}   % **** Enter the paper title and student information here

\maketitle
\thispagestyle{empty}

\section{Introduction}\label{sec:introduction}
This paper proposes a simple training strategy called MaxUp. The authors claim that MaxUp is embarrassingly simple and easy to apply while it still enhances performance. The central intuition behind MaxUp is similar to that of adversarial training, which is regularizing networks by inducing them to lie on a smooth loss landscape. MaxUp implements the smoothness regularization by generating a set of randomly augmented samples to calculate the worst-case loss for a given image and letting networks minimize the worst-case loss.

\section{Method}
Empirical Risk Minimization (ERM) is the standard scheme for training a regular neural network:
\begin{equation}
	\label{eq:1}
	\min_{\theta} \mathbb{E}_{x \sim \mathcal{D}_n} \left[ L (x; \theta)\right],
\end{equation}
where $\theta$ and $L$ denote the parameters of the network and the loss function, respectively, and $\mathcal{D}_n$ indicates the dataset while $x$ is a sampled data from $\mathcal{D}_n$. While ERM is simple, the authors argue that ERM often incurs networks' overfitting.

To ameliorate overfitting, the authors propose MaxUp, which induces the loss landscape around a sample to be smoother. More specifically, MaxUp randomly augments a given image to produce a set of transformed images. Then, MaxUp selects the worst-case loss among the losses calculated with the transformed images. Finally, MaxUp minimizes the worst-case loss as follows:
\begin{equation}
    \label{eq:2}
    \min_{\theta} \mathbb{E}_{x \sim \mathcal{D}_n} \left[ \max_{i \in [m]}L (x'_i; \theta)\right].
\end{equation}
where $x'_i$ denotes the $i$-th augmented sample. Eq.~\ref{eq:2}  is similar to the adversarial training in that both schemes first maximize loss using an augmentation and minimizes the maximzied loss. According to the paper, both training methods share similar smoothing effect.

\section{Experiments}
The paper conducts experiments on various datasets such as ImageNet, CIFAR10, and CIFAR100. Used network architectures include ResNet and EfficientNet. Across all the settings, MaxUp + CutMix shows slightly improved performance compared to Dropout, MixUp, and CutMix.

\section{Personal Opinion}
The proposed method requires multiple forward passes, which limits the usefulness of MaxUp. Moreover, MaxUp seems neither novel nor effective.


\end{document}
